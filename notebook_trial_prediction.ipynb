{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Trial Prediction Exercise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Objective"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to build a model able to predict if a user is going to become a paying user of the Lingokids service after its trial expires. Remember the trial is 7 days. \n",
    "We have four sources of data that can be classified into two broad categories\n",
    "1. Subscription data: This includes three different datasources\n",
    "    * Information related to onboarding: All the actions that the user take to create its profile\n",
    "    * Information related to subscription paywall: Here is where the user can opt to start a trial. It can be shown at different stages: it is always shown right after registration but it can also be shown after reaching daily limits or accesing limited features. If the user agrees to the trial is considered a succesful subscription and an invoice is charged after 7 days\n",
    "    * Information related to the billing: After 7 days of a succesful subscription a bill is sent to the user. This means that the user has become a paying user (the event we want to predict)\n",
    "2. Activity data: contains information about the activities that the user does within the app. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a \"sequence to class\" prediction problem where we have a sequence of events and we want to know if they will end up on an specific target variable (user becomes a paying user). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are many approaches to this problem. In our case we are going to aggregate all the different events that happen over time for each user and engineer the features that will form part of the prediction model. Engineering all these features will require a lot of data wrangling and cleaning. Remember that we want to predict as soon as possible if we are going to have a paying user. We need to start giving predictions from the beginning which could seem a contradiction with aggregating over time. However it can be done by establishing a process that listens to the stream of events and updates the aggregated metrics of the different users as they arrive. Every period of time in a batch manner we can evaluate the prediction models with those updated counters. With those predictions, decisions can be taken (send a discount/reminder if the probability of becoming a paying user is low, extend the trial, etc...)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. <a href='#onboard_data'>Onboard data</a>\n",
    "1. <a href='#subscription_data'>Subscription data</a>\n",
    "1. <a href='#invoice_data'>Invoice data</a>\n",
    "1. <a href='#activities_data'>Activities data</a>\n",
    "1. <a href='#create_dataset_and_training'>Create dataset and training</a>\n",
    "1. <a href='#evaluation'>Evaluation</a>\n",
    "1. <a href='#interpretability'>Interpretability</a>\n",
    "1. <a href='#conclusion'>Conclusion</a>\n",
    "1. <a href='#future_work'>Future work</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('max_colwidth', 200000)\n",
    "import json\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import roc_curve, plot_roc_curve, plot_precision_recall_curve\n",
    "from sklearn.model_selection import KFold\n",
    "import shap\n",
    "\n",
    "fpath_activities = 'data/activities.tsv'\n",
    "fpath_invoices = 'data/invoices.tsv'\n",
    "fpath_onboard_events = 'data/onboarding_events.tsv'\n",
    "fpath_subscription_events = 'data/subscription_events.tsv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <a id='onboard_data'>Onboard events data</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We suspect that the onboard events do not carry much information for this specific prediction problem. Mainly because it's a process that is mandatory to be able to use the application. However, it could be useful to know information like the level that the parent assigns to the child and if the signup process was succesful. (TODO: We did not include some other features that could characterise the user like the signup_provider or the age of the child)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# user_id\tID for a user.\n",
    "# session_id\tID for a session. Those events who don't have a session_id associated are sent from our server, normally as confirmation\n",
    "# event_at\tTimestamp of event. Timezone is UTC, not locale timezone of device\n",
    "# event_name\tName of the event. We capture different user behaviour by using different events when they perform certain actions.\n",
    "#           - onboarding_home: Start of onboarding, First screen\n",
    "#           - signup_level: Sent when a user has selected the english level.\n",
    "#                - level: Level selected. Three possible levels 2, 4 or 6\n",
    "#                - level_name:\n",
    "#           - signup_age: birthday in epoch format\n",
    "#           - signup_result: Event confirms registration.\n",
    "#               - child_id: id assigned for a child\n",
    "#               - success: boolean indicating if registering has been succesful\n",
    "#               - signup_provider. Method the user has used to register\n",
    "# data\tJSON of relevant data captured for this event. Each event_name has different information inside data (see Data tab)\n",
    "# context\tJSON of relevant data related to device, locale or other useful information, not related to specific action performed by user (see Context tab)\n",
    "onboard_events_df = pd.read_csv(fpath_onboard_events, sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onboard_events_df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set the types for the different columns with a function that will be applied across all dataframes in this exercise\n",
    "def set_dtypes_for_df(df, date_columns, categorical_columns, json_columns):\n",
    "    df[date_columns] = df[date_columns].astype('datetime64[ns]')\n",
    "    df[categorical_columns] = df[categorical_columns].astype(\"category\")\n",
    "    df[json_columns] = df[json_columns].astype(\"string\")\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onboard_events_df = set_dtypes_for_df(onboard_events_df, ['event_at'], ['event_name'], ['data','context'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Most of the users only have 4 onboarding events which makes sense since they are the ones that need to be fulfilled to start using the application\n",
    "onboard_events_df.groupby('user_id').count()['session'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's define a function that will count the number of events per user and pivot them as columns\n",
    "def column_count_values_and_pivot(df, column_group, column_count):\n",
    "    df = df.groupby([column_group, column_count]).count().reset_index(\n",
    "        level=column_count).pivot(columns=column_count, values=df.columns[-2])\n",
    "    df.columns = [str(column_count) + '_' + str(x) for x in df.columns.to_list()]\n",
    "    return df.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onboard_events_count_df = column_count_values_and_pivot(onboard_events_df, 'user_id', 'event_name')\n",
    "onboard_events_count_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's also extract the info out of the jsons in the different event types\n",
    "# we will use an auxiliary function to extract the information that we need in the jsons\n",
    "def append_json_information(df, json_column, fields):\n",
    "    df.loc[:, json_column] = df[json_column].astype(\"string\")\n",
    "    for field in fields:\n",
    "        df.loc[:, field] = df[json_column].apply(lambda x: json.loads(x)[field])        \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take the success field out of the data JSON and eliminate any duplicates by grouping the user id\n",
    "onboard_success_df = append_json_information(onboard_events_df[onboard_events_df['event_name']=='signup_result'], 'data', ['success']).groupby('user_id').agg({'success' : 'first'}).rename(columns={'success' : 'onboard_success'})\n",
    "# same with the english level of the child\n",
    "onboard_level_df = append_json_information(onboard_events_df[onboard_events_df['event_name']=='signup_level'], 'data', ['level']).groupby('user_id').agg({'level' : 'first'}).rename(columns={'level' : 'onboard_level'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finally, put together all the dfs to generate the features of the onboard_events\n",
    "dfs = [onboard_events_count_df, onboard_level_df, onboard_success_df]\n",
    "onboard_events_features = reduce(lambda left, right: left.join(right, how='outer'), dfs )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Any NA's after the join?\n",
    "onboard_events_features.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "622 users didn't complete the onboarding successfully. It makes sense to ignore this users since they will never get to the 7 days trial. We will drop them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onboard_events_features = onboard_events_features.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <a id='subscription_data'>Subscription data</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This events are related to the paywall that is shown to the user and where she decides if she wants to start a trial or not. It contains valueable information because it will show us how many times the user postpones the trial or if he goes for the trial straight away after the onboarding process. We will count the number of subscription enter events that the user sees, the source of these events and if the subscription was succesful or not. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# user_id\tID for a user.\n",
    "# session_id\tID for a session. Those events who don't have a session_id associated are sent from our server, normally as confirmation\n",
    "# event_at\tTimestamp of event. Timezone is UTC, not locale timezone of device\n",
    "# event_name\tName of the event. We capture different user behaviour by using different events when they perform certain actions.\n",
    "#       - subscription_enter: When a user enters in paywall.\n",
    "#           - source: Which user flow has triggered. postonboarding (shown when user finishes on boarding) or launcher\n",
    "#           - child_id: The id of the child\n",
    "#           - platform\n",
    "#       - subscription_succesful: Confirm subscription.\n",
    "#           - subscription_id: id for subscription\n",
    "#           - price: the price of the subscription\n",
    "#           - currency:\n",
    "#           - payment platform\n",
    "# data\tJSON of relevant data captured for this event. Each event_name has different information inside data (see Data tab)\n",
    "# context\tJSON of relevant data related to device, locale or other useful information, not related to specific action performed by user (see Context tab)\n",
    "subscription_events_df = pd.read_csv(fpath_subscription_events, sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_events_df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_events_df = set_dtypes_for_df(subscription_events_df, ['event_at'], ['event_name'], ['data','context'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Many users only have 2 subscription events which makes sense since they are the ones that need to be fulfilled to start using the application (subscription_enter[postonboarding] followed by subscription_succesful)\n",
    "subscription_events_df.groupby('user_id').count()['event_name'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Counting the number of times a user enters the subscription event"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_events_count_df = column_count_values_and_pivot(subscription_events_df, 'user_id', 'event_name')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_events_count_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Some users have more than one subscription succesful which does not really makes sense. We change values greater than one to one\n",
    "subscription_events_count_df['event_name_subscription_successful'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_events_count_df.loc[subscription_events_count_df['event_name_subscription_successful']>1, 'event_name_subscription_successful'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_events_count_df['event_name_subscription_successful'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Before continuing, there is a source in the jsons that is an empty dict\n",
    "subscription_enter_source_df = append_json_information(subscription_events_df[subscription_events_df['event_name']=='subscription_enter'], 'data', ['source'])\n",
    "subscription_enter_source_df = subscription_enter_source_df[~(subscription_enter_source_df['source'] == {})]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_enter_source_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_enter_source_df = column_count_values_and_pivot(subscription_enter_source_df, 'user_id', 'source')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_enter_source_df.columns = ['subscription_enter_' + str(col) for col in subscription_enter_source_df.columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_enter_source_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We will extract the subscription_id as we will need it to correlate it with invoices\n",
    "subscription_id_df = append_json_information(subscription_events_df[subscription_events_df['event_name']=='subscription_successful'], 'data', ['subscription_id'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# There are users with two subscriptions. Not sure if this is intended, so we are going to just keep the first time they subscribed\n",
    "subscription_id_df = subscription_id_df.sort_values('event_at').drop_duplicates(subset='user_id', keep=\"first\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_id_df = subscription_id_df.set_index(subscription_id_df['user_id'])[['subscription_id']]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs = [subscription_events_count_df, subscription_enter_source_df, subscription_id_df]\n",
    "subscription_events_features = reduce(lambda left, right: left.join(right, how='outer'), dfs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_events_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <a id='invoice_data'>Invoice data</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The invoice data is going to be our target variable. We want to predict if the user is going to generate an invoice or not. In other words, we want to know if a user is going to become a customer of the lingokids service"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "invoices_df = pd.read_csv(fpath_invoices, sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "invoices_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# There is not much preprocessing for the invoices, just converting the dates to datetime, renaming the columns and droping duplicates\n",
    "invoices_df['purchased_at'] = invoices_df['purchased_at'].astype('datetime64[ns]')\n",
    "invoices_df = invoices_df.sort_values('purchased_at').drop_duplicates(subset='subscription_id', keep=\"first\")\n",
    "invoices_df.columns = ['invoices_' + str(col) for col in invoices_df.columns]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "invoices_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We can already merge it with the subscription data\n",
    "# save the index since merge eliminates the index\n",
    "ix = subscription_events_features.index\n",
    "subscription_events_features = subscription_events_features.merge(invoices_df, left_on='subscription_id', right_on='invoices_subscription_id', how='left')\n",
    "subscription_events_features = subscription_events_features.set_index(ix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subscription_events_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <a id='activities_data'>Activities data</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The activities data is also very useful. It will allow us to measure what's the interaction of the user with the app. Do they play a lot?. Do they complete a lot of activities?. From where do they play? "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# user_id\tID for a user.\n",
    "# session\tID for a session.\n",
    "# event_at\tTimestamp of event. Timezone is UTC, not locale timezone of device\n",
    "# source\tOrigin where activity has been launched\n",
    "# activity\tId of activity\n",
    "# activity_name\tName of activity\n",
    "# type\tType of activity\n",
    "# subtype\tSubtype of activity\n",
    "# child_id\tId assigned for a child. An user may have more than one child associated\n",
    "# duration\tNumber of seconds spent on activity\n",
    "# completed\tActivity has been completed? An user may exit without compliting an activity\n",
    "# game_completed\tSame as completed but only for activities whose type is 'game'\n",
    "# downloading_time\tSeconds spent in download info needed\n",
    "# loading_time\tSeconds spent in loading the activity in the app\n",
    "# replay_times\tNumber of times activity has been played so far\n",
    "# os\tJSON with information relevant to operating system of device (see Context below)\n",
    "# location\tJSON with information relevant to location of user (see Context below)\n",
    "# timezone\tTimezone. Needed if you need to translate timestamp UTC to timestamp locale\n",
    "# locale\tLanguage of device\n",
    "# device\tJSON with information about device (see Context below)\n",
    "activities_df = pd.read_csv(fpath_activities, sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activities_df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activities_df = set_dtypes_for_df(activities_df, ['event_at'], ['source', 'activity', 'activity_name', 'type', 'subtype', 'completed', 'game_completed', 'timezone', 'locale'], ['os','location'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activities_df.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# There is one row that has only NaN values\n",
    "activities_df[activities_df['location'].isnull()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Eliminate this one\n",
    "activities_df = activities_df[~activities_df['location'].isnull()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get out of the jsons information that could be important like the name of the os and the location of the activity\n",
    "activities_df['activity_os_name'] = append_json_information(activities_df, 'os', ['name'])['name'].astype('category')\n",
    "activities_df['activity_location'] = append_json_information(activities_df, 'location', ['country'])['country'].astype('category')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activities_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We are now ready to aggregate all this events by user. lets check null values first\n",
    "activities_df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Game completed and subtype missing values are not important. We won't use them in our prediction model because of their high cardinality (curse of dimensionality) and game completed is a duplicate of completed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nulls for source. Which ones?\n",
    "activities_df[activities_df['source'].isna()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Impute them with the most common values\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "activities_df['source'] = imp.fit_transform(activities_df[['source']])[:, 0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# nulls for location. Which ones?\n",
    "# They are in the Asia and Africa timezone but we cannot deduct the country from here. We will drop them as the country is a very important piece of information that describes the user behaviour\n",
    "activities_df = activities_df[~activities_df['activity_location'].isna()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activities_df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We are going to drop the following features. Activity name because it is a duplicate of activity and subtype because is a subset of type\n",
    "# child_id, game_completed because is a duplicate of completed, os and location (json format), timezone, locale, os_version we are going to assume that do not influence in the result\n",
    "columns_to_drop = ['session', 'activity_name', 'subtype', 'child_id', 'game_completed', 'os', 'location', 'timezone', 'locale', 'name', 'country']\n",
    "activities_df = activities_df.drop(columns_to_drop, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activities_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We start aggregating all of this information by user id\n",
    "# First the numeric values\n",
    "numeric_fields_activities_df = activities_df.groupby('user_id').agg({'duration': 'sum',\n",
    "                                                                    'downloading_time': 'sum',\n",
    "                                                                    'loading_time': 'sum',\n",
    "                                                                    'replay_times': 'sum'\n",
    "                                                                    })\n",
    "numeric_fields_activities_df.columns = ['activities_' + col for col in numeric_fields_activities_df.columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Then the activity categories (location and os name) which are suppose to be unique so we keep just one\n",
    "unique_fields_activities_df = activities_df.groupby('user_id').agg({'activity_location': 'first',\n",
    "                                                                   'activity_os_name': 'first'\n",
    "                                                                   })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_fields_activities_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create counters for the source of the activity. We suspect that past activities is a good predictor as it means that a previous activity was of interest to the user. Same with the number of activities launched"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "source_activity_count_df = column_count_values_and_pivot(activities_df, 'user_id', 'source')\n",
    "source_activity_count_df.columns = ['activity_' + col for col in source_activity_count_df.columns]\n",
    "source_activity_count_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The type of activity could also be important. Users that experience the full range of activities might be more attracted to the app."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type_activity_count_df = column_count_values_and_pivot(activities_df, 'user_id', 'type')\n",
    "type_activity_count_df.columns = ['activity_' + col for col in type_activity_count_df.columns]\n",
    "type_activity_count_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The number of activities completed or interrupted is also an important insight. Do they play until the end and complete activities or just get tired of it?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "completed_activity_count_df = column_count_values_and_pivot(activities_df, 'user_id', 'completed')\n",
    "completed_activity_count_df.columns = ['activity_' + col for col in completed_activity_count_df.columns]\n",
    "completed_activity_count_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# join all the features into the activity features\n",
    "dfs = [completed_activity_count_df, type_activity_count_df, source_activity_count_df, unique_fields_activities_df, numeric_fields_activities_df]\n",
    "activity_features_df = reduce(lambda left, right: left.join(right, how='outer'), dfs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activity_features_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <a id='create_dataset_and_training'>Create Dataset and Training</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To create the dataset we join the activity features, the subscription events features and the onboard events features. Remember that the last one contains information of wether the user generated an invoice or not"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs = [activity_features_df, onboard_events_features, subscription_events_features ]\n",
    "dataset = reduce(lambda left, right: left.join(right, how='outer'), dfs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Now that we have joined the dataset let's check the nans again as the joining process could leave some fields with NaN value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Several things to take into account when dealing with the NaN's\n",
    "- Activities counts will be filled with 0 as a NaN value means absence of playing that particular activity. Same with duration, downloading time, loading time, etc...\n",
    "- Subscription enter counted events will be filled with the most frequent values as we assume that the user follows the normal procedure of subscription\n",
    "- Invoices values nans will be engineered to be our target variable. A nan means that there is no invoice and the customer did not become a paying customer. \n",
    "- Most of the users subscribed succesfully. Two of them didn't but we will drop them\n",
    "- For the users that somehow did not complete the onboarding process, we will impute them the most common values and assume that they followed the same onboarding process as most users\n",
    "- The rest of categories like the os_name and location or the child level in the onboarding process will be imputed with the most common values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Many users didn't go through the onboarding process (2714 users). Let's assume that they went through the same onboarding process as the majority of users and impute most frequent values\n",
    "features_to_impute = ['event_name_onboarding_home','event_name_signup_age', 'event_name_signup_level', 'event_name_signup_result', 'onboard_level', 'onboard_success']\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "dataset[features_to_impute] = imp.fit_transform(dataset[features_to_impute])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Activities counts will be filled with 0 as a NaN value means absence of playing that particular activity. Same with duration, downloading time, loading time, etc... However we must impute something into the activity os name and the location\n",
    "features_to_impute = ['activity_os_name', 'activity_location']\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "dataset[features_to_impute] = imp.fit_transform(dataset[features_to_impute])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop those two users without a succesful registration (aparently they just did the onboarding and we are interested on knowing if a registered user is going to become a paying user)\n",
    "dataset = dataset[~dataset['event_name_subscription_successful'].isna()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# some users do not have a subscriptionid. They didn't enter the subscription succesful event either so we drop them too\n",
    "dataset = dataset[~dataset['subscription_id'].isna()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_to_impute = ['subscription_enter_source_launcher','subscription_enter_source_parents', 'subscription_enter_source_postonboarding', 'subscription_enter_source_stickeralbum', 'subscription_enter_source_upsell_download_modal']\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "dataset[features_to_impute] = imp.fit_transform(dataset[features_to_impute])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's create the output variable. If there is no data for refunded_invoice then it's not a paying user \n",
    "dataset['paying_customer'] = 1\n",
    "dataset.loc[dataset['invoices_purchased_at'].isna(), 'paying_customer'] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# And as a final step, drop those features that do not bring any value and fill the previous counts that are numeric\n",
    "features_to_drop = ['invoices_subscription_id', 'subscription_id', 'invoices_purchased_at', 'invoices_refunded_invoice']\n",
    "dataset = dataset.drop(features_to_drop, axis=1)\n",
    "dataset.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the types of the dataset\n",
    "categorical_fields = ['activity_location', 'activity_os_name', 'onboard_success', 'onboard_level', 'paying_customer']\n",
    "dataset[categorical_fields] = dataset[categorical_fields].astype(\"category\")\n",
    "# the onboard level is going to be change to a string to not be mistaken with a numeric value\n",
    "dataset['onboard_level'] = dataset['onboard_level'].cat.rename_categories([\"low\", \"medium\", \"high\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's train the model. We are goint to use CatBoost. Ensemble methods are a good way of reducing noise, bias and variance. Boosting is one of these methods and Catboost gives us an implementation that works well out of the box and that outputs and interpretable model. We also do not have to deal with normalisation and correlated features, so the data preprocessing is reduced even further"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = dataset.drop(['paying_customer'], axis=1)\n",
    "categorical_indexes = [X.columns.get_loc(c) for c in X.select_dtypes('category').columns]\n",
    "Y = dataset['paying_customer']\n",
    "kfold = KFold(n_splits=2)\n",
    "roc_curve_scores = []\n",
    "for train_index, test_index in kfold.split(dataset):\n",
    "    x_train, x_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "    clf = CatBoostClassifier()\n",
    "    clf.fit(x_train, y_train, cat_features=categorical_indexes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <a id='evaluation'>Evaluation</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a binary classification task (customer will pay or not). One of the ways to evaluate this is through a ROC curve, which measures the performance of a classifier when using different probabilty thresholds to classify an instance into one of the two options. We obviously want to get right our predictions (true positives) but we don't want to wrongly classify a user as a paying customer (false positives). The balance between the two depends on the use case. For this kind of use case a false positive does not have severe consequences, so we can afford a relatively high number of them. On the other hand, in other use cases such as cancer detection, we cannot afford many false positives. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_roc_curve(clf, x_test, y_test) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our classifier has a AUC of 0.65. That means that is better than a random guess which can already provide value. It could be improved by also considering the time dimension (did users play a lot the first few days or consistently during the trial?). We leave that as future work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "precision-recall is also used when working with unbalanced datasets (not our case, see below). In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(clf, x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the problem is not unbalanced\n",
    "dataset['paying_customer'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <a id='interpretability'>Interpretability</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An important thing is also to interpret the model. It can provide us insights on which factors are more important on our customer churn and what are the focus points. For this study we will use ShapValues which works nicely with the CatBoost library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap_values = clf.get_feature_importance(Pool(x_train, label=y_train,cat_features=categorical_indexes),\n",
    "                                                                     type=\"ShapValues\")\n",
    "shap_values = shap_values[:,:-1]\n",
    "shap.summary_plot(shap_values, x_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see the impact of the most important attributes in the chart. If the points are in red it means high values for the feature. If they are to the left it means that they affect negatively the paying user possibility and viceversa. Some insights: \n",
    "- We can see that surprisingly, those users that are shown the paywall after onboarding are less likely to become paying customers. This could be misleading as not many users are not shown the paywall after onboarding and therefore, might be just a misinterpretation. More data will clarify this point\n",
    "- The more the user plays, the more likely he is going to become a paying customer. Same with loading time\n",
    "- Paywalls shown in the parents area have a bigger impact on turning a customer into a paying one\n",
    "- Users that play past activities are most likely to become customers. \n",
    "- Onboard level also has an impact. Seems that users that put their children into the higher levels are most likely to become paying customers\n",
    "- Users of some countries are more likely to pay for the app\n",
    "\n",
    "See more plots below to understand better the impact of the variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also include a correlation matrix. We can see that the higher correlations are usage of the app with more activities completed and the duration of the usage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cor_dataset = dataset\n",
    "cor_dataset['paying_customer'] = cor_dataset['paying_customer'].astype('float')\n",
    "cor_dataset.corr()[cor_dataset.corr()['paying_customer']>0.1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filter_by_top_n(data, n, category_name):\n",
    "    top_n_categories = data[category_name].value_counts().nlargest(n).index\n",
    "    result_df = data[data[category_name].isin(top_n_categories)]\n",
    "    result_df[category_name].cat.set_categories(result_df[category_name].unique(), inplace=True)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def plot_top_n(data, n, category):\n",
    "    return sns.countplot(x=category, data=filter_by_top_n(data, n, category), hue='paying_customer')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Users of some countries are more likely to pay for the app\n",
    "plot_top_n(dataset, 10, 'activity_location')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(data=dataset, x='subscription_enter_source_postonboarding', hue='paying_customer' )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clearly the duration is bigger with paying customers\n",
    "sns.barplot(x=\"paying_customer\", y=\"activities_duration\", data=dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(data=dataset, x='activity_os_name', hue='paying_customer' )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.countplot(data=dataset, x='subscription_enter_source_parents', hue='paying_customer' )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# zoom into the paywalls shown more than one time in the parents section. They have a greater conversion rate\n",
    "sns.countplot(data=dataset[dataset['subscription_enter_source_parents']> 0], x='subscription_enter_source_parents', hue='paying_customer' )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Children in higher levels are more likely to become paying users\n",
    "sns.countplot(data=dataset, x='onboard_level', hue='paying_customer' )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.barplot(x=\"paying_customer\", y=\"activity_source_pastActivities\", data=dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.barplot(x=\"paying_customer\", y=\"event_name_onboarding_home\", data=dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.barplot(x=\"paying_customer\", y=\"activity_type_game\", data=dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <a id='conclusion'>Conclusion</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In most of the data science projects, a big deal of the time is spent on preparing the data. This was no exception as there was a lot of data massaging and wrangling involved in engineering features out of the dataset, which took most of the time of this exercise. \n",
    "There are indeed several factors that influence wether the user will become a paying customer. In general, greater usage times and more activities completed help to become a paying customer. But also some factors like the location or information about the onboarding process can provide inmmediate information, without waiting for the customer to use the app further. The OS is also an important factor. Finally, some paywalls are more effective that others, specially those shown in the parents section. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <a id='future_work'>Future Work</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that here we are aggregating the events for each customer during the whole trial. We could aggregate this events for day 1, day 2, etc... of the trial, as the impact of one variable in day 1 does not have to be the same as in the last days of the trial. \n",
    "Another point to improve is the model. Since this is a sequence prediction problem, models like LTSM could be more effective since they are able to memorise the order of the events instead of just aggregating them without considering their sequence. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}